---
title: Fairness of AI in Medical Imaging
description: "MICCAI 2023 Workshop"
layout: default
permalink: /2023-miccai/index.html
weight: -1
redirect_from: /2023-miccai/
---

## TL;DR

 - Fairness & Medical Imaging workshop on **Oct 12th** at [MICCAI 2023](https://conferences.miccai.org/2023/en/) (Vancouver)
 - Organized jointly with the workshop on [Ethical and Philosophical Issues in Medical Imaging](https://sites.google.com/view/epimi)
 - The proceedings book will be freely available from 7 October 2023: https://link.springer.com/book/9783031452482 
 - Also see [our separate, free online event on Nov 6th](/2023-online/) and [last year's iteration of the same event](/2022/)


## Keynote - Dr. Judy Gichoya
#### “Shortcuts” Causing Bias in Radiology Artificial Intelligence: Causes, Evaluation, and Mitigation
<p style="text-align: justify">Despite the expert-level performance of artificial intelligence (AI) models for various medical imaging tasks, real-world performance failures with disparate outputs for various subgroups limit the usefulness of AI in improving patients’ lives. 
Many definitions of fairness have been proposed, with discussions of various tensions that arise in the choice of an appropriate metric to use to evaluate bias; for example, should one aim for individual or group fairness? 
One central observation is that AI models apply “shortcut learning” whereby spurious features (such as chest tubes and portable radiographic markers on intensive care unit chest radiography) on medical images are used for prediction instead of identifying true pathology.
Moreover, AI has been shown to have a remarkable ability to detect protected attributes of age, sex, and race, while the same models demonstrate bias against historically underserved subgroups of age, sex, and race in disease diagnosis.
Therefore, an AI model may take shortcut predictions from these correlations and subsequently generate an outcome that is biased toward certain subgroups even when protected attributes are not explicitly used as inputs into the model.
As a result, these subgroups became nonprivileged subgroups. 
In this talk, Dr. Gichoya will discuss the various types of bias from shortcut learning that may occur at different phases of AI model development, including data bias, modeling bias, and inference bias. 
She will also discuss mitigation strategies to bias arising from "shortcuts."
</p>

<div class="clearfix">
	<img class="img2" src="/assets/speakers/Gichoya.jpg" alt="Dr. Judy Gichoya" width="150" style="float: left; padding:0px 10px 10px 10px">
	<p style="text-align: justify">
		<b>Dr. Judy Gichoya</b> is an assistant professor at Emory university in Interventional Radiology and Informatics. 
		Her career focus is on validating machine learning models for health in real clinical settings, exploring explainability, fairness, and a specific focus on how algorithms fail. 
		She is heavily invested in training the next generation of data scientists through multiple high school programs, serving as the program director for radiology: AI trainee editorial board and the medical students machine learning elective.
	</p>
</div>

## Schedule for FAIMI/EPIMI Workshop

 | Time | Speaker and Title |
 |------|-------- |
 |**FAIMI** ||
 | 1:30 - 1:35 | Welcome | 
 | 1:35 - 2:20 | **Keynote speaker:** Dr. Judy Gichoya  - “Shortcuts” Causing Bias in Radiology Artificial Intelligence: Causes, Evaluation, and Mitigation |
 | 2:20 - 2:35 | Nilesh Kumar, et al.: Distributionally Robust Optimization and Invariant Representation Learning for Addressing Subgroup Underrepresentation: Mechanisms and Limitations | 
 | 2:35 - 2:50 | Thorsten Kalb, et al.: Revisiting Skin Tone Fairness in Dermatological Lesion Classification |
 | 2:50 - 3:05 | Carolina Piçarra, et al.: Analysing race and sex bias in brain age prediction |
 | 3:05 - 3:15 | Poster pitch | 
 | 3:15 - 4:00 | Posters/Coffee | 
 | 4:00 - 4:15 | Cosmin I. Bercea, et al.: Bias in Unsupervised Anomaly Detection in Brain MRI
 | 4:15 - 4:30 | Nicolás Gaggion, et al.: Unsupervised bias discovery in medical image segmentation
 | 4:30 - 4:45 | Amar Kumar, et al.: Debiasing Counterfactuals In the Presence of Spurious Correlations 
 |**EPIMI** ||
 | 4:45 - 4:50 | Welcome | 
 | 4:50 - 5:15 | On the Relationship between Open Science in Artificial Intelligence for Medical Imaging and Global Health Equity |
 | 5:15 - 5:40 | Gradient-based enhancement attacks in biomedical machine learning |
 | 5:40 - 5:45 | Final questions and open discussion |
 |**FAIMI and EPIMI** ||
 | 5:45 - 6:00 | Prizes and closing for FAIMI and EPIMI| 

 
## Accepted Papers
Chenwei Wu, et al.: *De-identification and Obfuscation of Gender Attributes From Retinal Scans*

Yuning Du, et al.: *Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction*  

Sophie A. Martin, et al.: *Brain matters: Exploring bias in AI for neuroimaging research*  

Cosmin I. Bercea, et al.: *Bias in Unsupervised Anomaly Detection in Brain MRI*  

María Agustina Ricci Lara, et al.: *Towards Unraveling Calibration Biases in Medical Image Analysis*  

Nina Weng, et al.: *Are Sex-based Physiological Differences the Cause of Gender Bias for Chest X-ray Diagnosis?*  

Rebecca S. Stone, et al.: *Bayesian uncertainty-weighted loss for improved generalisability on polyp segmentation task*  

Yun-Yang Huang, et al.: *Mitigating Bias in MRI-Based Alzheimer's Disease Classifiers through Pruning of Deep Neural Networks*  

Vien N Dang, et al.: *Auditing Unfair Biases in CNN-based Diagnosis of Alzheimer’s Disease*  

Nilesh Kumar, et al.: *Distributionally Robust Optimization and Invariant Representation Learning for Addressing Subgroup Underrepresentation: Mechanisms and Limitations*  

Carolina Piçarra, et al.: *Analysing race and sex bias in brain age prediction*  

Mahsa Dibaji, et al.: *Studying the Effects of Sex-related Differences on Brain Age Prediction using brain MR Imaging*  

Tiarna Lee, et al.: *An investigation into the impact of deep learning model choice on sex and race bias in cardiac MR segmentation*  

Mohamed Huti, et al.: *An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features*  

Dewinda J. Rumala: *How You Split Matters: Data Leakage and Subject Characteristics Studies in Longitudinal Brain MRI Analysis*  

Thorsten Kalb, et al.:, *Revisiting Skin Tone Fairness in Dermatological Lesion Classification*  

Ario Sadafi, et al.:, *A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes*  

Nicolás Gaggion, et al.:, *Unsupervised bias discovery in medical image segmentation*  

Amar Kumar, et al.:, *Debiasing Counterfactuals In the Presence of Spurious Correlations*  


## Call for Papers
We invite the submission of papers for

<p style="text-align: center;"><b>FAIMI: The MICCAI 2023 Workshop on Fairness of AI in Medical Imaging</b>.</p>

<p style="text-align: justify">Over the past several years, research on fairness, equity, and accountability in the context of machine learning has extensively demonstrated ethical risks in the deployment of machine learning systems in critical infrastructure, such as medical imaging.
The FAIMI workshop aims to encourage and emphasize research on and discussion of fairness of AI within the medical imaging domain.
We therefore invite the submission of papers, which will be selected for oral or poster presentation at the workshop. 
Topics include but are not limited to:</p>
- Case studies providing evidence of and/or addressing bias in medical imaging
- Algorithmic fairness in medical imaging
- Methods for measuring/evaluating bias in medical imaging
- Methods for mitigating bias in medical imaging
- Ethical aspects of bias and fairness in medical imaging
- Legal aspects of bias and fairness in medical imaging
- Policy in the context of bias and fairness in medical imaging

<p style="text-align: justify">The workshop proceedings will be published in the MICCAI workshops volumes of the Springer Lecture Notes Computer Science (LNCS) series. 
Selected papers will also be invited to present at <a href="/2023-online/">the virtual FAIMI workshop tentatively scheduled for November 6th</a>.
Papers should be <i>anonymized</i> and at most 8 pages plus at most 2 extra pages of references using the <a href="https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines">LNCS format</a>. 
The review process is conducted in a double-blind manner, following MICCAI standards. 
Submissions are made in <a href="https://cmt3.research.microsoft.com/FAIMI2023">CMT</a>. 
</p>

<p style="text-align: justify">
Following the <a href="https://conferences.miccai.org/2023/en/PAPER-SUBMISSION-AND-REBUTTAL-GUIDELINES.html">MICCAI paper submission guidelines</a>, the submission of additional <b>supplementary material</b> is possible.
This should be a separate file, and reviewers are under no obligation to review it; the paper must be self-contained and understandable without the supplementary material.
Note that in the submission system, supplementary materials can only be added once a regular submission has been created. (You can still edit your submission until the deadline.)
</p>


## Dates

*All dates are [Anywhere on Earth](https://en.wikipedia.org/wiki/Anywhere_on_Earth).*

**~~July 21st~~ July 28th, 2023**: Paper submission

**~~August 11th~~ August 9th, 2023**: Notification of paper decisions

**~~August 25th~~ August 18th, 2023**: Camera-ready deadline

**October 12th, 2023**: Workshop


## Organizers

**Aasa Feragen**, DTU Compute, Technical University of Denmark  
**Andrew King**, King's College London  
**Ben Glocker**, Imperial College London  
**Daniel Moyer**, Vanderbilt University  
**Enzo Ferrante**, CONICET, Universidad Nacional del Litoral  
**Eike Petersen**, DTU Compute, Technical University of Denmark  
**Esther Puyol-Antón**, HeartFlow and King's College London  
**Melanie Ganz-Benjaminsen**, University of Copenhagen & Neurobiology Research Unit, Rigshospitalet  
**Veronika Cheplygina**, IT University Copenhagen  

## Contact

Please direct any inquiries related to the workshop or this website to <a href="mailto:faimi-organizers@googlegroups.com">faimi-organizers@googlegroups.com</a>.
